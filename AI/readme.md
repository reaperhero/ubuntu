

第一阶段：听课，吴恩达或李宏毅B站课，二选一即可，不需要完全搞明白，知道一些主要思想，深度神经网络大概原理，简单做一些实验

第二阶段：李沐的动手学深度学习或者Karpathy大神的zero-to-hero系列课程  时间不足就看李沐老师的即可

第三阶段：完成前两个阶段可以说彻底入门了，现在主流模型仍然是Transformer架构的，第三阶段就是吃透这个架构中新的一些点，如旋转位置编码，分组注意力、kvcache等。阅读一些经典论文，听别人讲的再清晰都不让自己逐字逐句的把这些经典论文读下来记忆更深刻，如Bert、Clip、VIT等，李沐有个系列是带读论文，都是很经典的论文，推荐有时间找几篇方向合适的视频看一遍，多模态领域clip以及vit必看。

第四阶段：前面第三阶段已经学习了VIT及CLIP，那主流多模态模型学习就水到渠成了，当前主流的多模态（默认文字+图片）就是在llm前面加上一个vit视觉层。这个阶段可以追一些最新模型技术报告和代码了，如qwenvl系列等，去huggingface上查找即可，把代码顺一遍，读读技术报告，再跑一下部署等即可，一定要多看一些最新模型，更加系统理解。


第五阶段：这个就开始分化了，看实际想做什么方向，底座基础前面都打好，如果想做AI训练微调就重点去看PPO、DPO、GRPO相关，训练方向一定要系统学一下强化学习，现在强化学习是必备，强化学习可以看我主页里的文章，有明确的学习路线。如果想做AI应用就去接触RAG、智能体等方向，github上比较火的太多了：langchain、llamaindex、dspy、crewai等等等等。如果想做模型部署可以看llama.cpp，sglang、vllm项目等。如果想纯研究模型架构优化，这个方向学历不高不推荐搞，太学术性，如模型稀疏化（MOE、NSA、MLA）、模型量化：awq、gguf、gptq、KVcache量化稀疏化、模型编译优化等等。剩下的还有很多，就不一一列举了，相信完成前四个阶段大家就很明确的知道自己想做的方向了。

第六阶段：追最新论文，一直追，无尽头的追


